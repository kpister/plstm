{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sys import exit\n",
    "from numpy import array\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, LSTM, Bidirectional, Embedding\n",
    "\n",
    "# following https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/\n",
    "# and       https://github.com/enriqueav/lstm_lyrics/blob/master/classifier_train.py\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 30\n",
    "\n",
    "def load_doc(filename):\n",
    "    with open(filename) as f:\n",
    "        text = f.read()\n",
    "\n",
    "    words = [word.strip() for word in text.split('\\n')]\n",
    "    return {'words': words, 'text': text}\n",
    "\n",
    "def shuffle_and_split_training_set(sentences_original, labels_original, percentage_test=10):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "    tmp_sentences = []\n",
    "    tmp_next_char = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_char.append(labels_original[i])\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_char[:cut_index], tmp_next_char[cut_index:]\n",
    "\n",
    "    print(\"Training set = %d\\nTest set = %d\" % (len(x_train), len(y_test)))\n",
    "    return array(x_train), array(y_train), array(x_test), array(y_test)\n",
    "\n",
    "# Data generator for fit and evaluate\n",
    "def generator(word_list, labels_list, batch_size, mapping):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQ_LENGTH), dtype=np.int32)\n",
    "        y = np.zeros((batch_size), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(word_list[index % len(word_list)]):\n",
    "                x[i, t] = mapping[w]\n",
    "            y[i] = labels_list[index % len(word_list)]\n",
    "            index = index + 1\n",
    "        yield x, y\n",
    "\n",
    "def get_model(vocab_size):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(75, input_shape=(SEQ_LENGTH, vocab_size)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = load_doc('names_few_30.txt')\n",
    "normal   = load_doc('double_words_few_30.txt')\n",
    "\n",
    "# mapping from chars to nums\n",
    "chars = sorted(list(set(proteins['text'] + normal['text'])))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "vocab_size = len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Training set = 78399\n",
      "Test set = 8712\n"
     ]
    }
   ],
   "source": [
    "# convert each character to an array of one hot encoded vectors\n",
    "sequences = list()\n",
    "for line in (proteins['words'] + normal['words']):\n",
    "    sequences.append(array([mapping[char] for char in line]))\n",
    "\n",
    "sequences = array(sequences)\n",
    "sequences = array([to_categorical(x, num_classes=vocab_size) for x in sequences]) \n",
    "X = np.empty([array(sequences).shape[0], sequences[0].shape[0], sequences[0].shape[1]])\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    try:\n",
    "        X[i] = seq\n",
    "    except:\n",
    "        pass     # where is this error coming from?\n",
    "\n",
    "# create datasets\n",
    "y = [1]*len(proteins['words']) + [0]*len(normal['words'])\n",
    "y = to_categorical(y, num_classes=2)\n",
    "x_train, y_train, x_test, y_test = shuffle_and_split_training_set(X, y)\n",
    "y_train = y_train\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 75)                93900     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 152       \n",
      "=================================================================\n",
      "Total params: 94,052\n",
      "Trainable params: 94,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 78399 samples, validate on 8712 samples\n",
      "Epoch 1/10\n",
      " - 88s - loss: 0.2075 - acc: 0.9176 - val_loss: 0.1306 - val_acc: 0.9558\n",
      "Epoch 2/10\n",
      " - 80s - loss: 0.0966 - acc: 0.9676 - val_loss: 0.0800 - val_acc: 0.9781\n",
      "Epoch 3/10\n",
      " - 81s - loss: 0.0701 - acc: 0.9763 - val_loss: 0.0612 - val_acc: 0.9797\n",
      "Epoch 4/10\n",
      " - 82s - loss: 0.0560 - acc: 0.9814 - val_loss: 0.0540 - val_acc: 0.9803\n",
      "Epoch 5/10\n",
      " - 82s - loss: 0.0466 - acc: 0.9852 - val_loss: 0.0512 - val_acc: 0.9846\n",
      "Epoch 6/10\n",
      " - 81s - loss: 0.0407 - acc: 0.9873 - val_loss: 0.0435 - val_acc: 0.9859\n",
      "Epoch 7/10\n",
      " - 80s - loss: 0.0369 - acc: 0.9885 - val_loss: 0.0400 - val_acc: 0.9874\n",
      "Epoch 8/10\n",
      " - 81s - loss: 0.0326 - acc: 0.9898 - val_loss: 0.0372 - val_acc: 0.9882\n",
      "Epoch 9/10\n",
      " - 80s - loss: 0.0288 - acc: 0.9910 - val_loss: 0.0366 - val_acc: 0.9890\n",
      "Epoch 10/10\n",
      " - 82s - loss: 0.0271 - acc: 0.9915 - val_loss: 0.0324 - val_acc: 0.9901\n"
     ]
    }
   ],
   "source": [
    "model = get_model(vocab_size)\n",
    "\n",
    "# fit model\n",
    "model.fit(x_train, y_train, epochs=10, verbose=2, validation_data=(x_test, y_test))\n",
    "\n",
    "# save the model to file\n",
    "model.save('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8712/8712 [==============================] - 3s 315us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03237777702835657, 0.9901285583103765]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWord(word, model, truth):\n",
    "    line = (word + \"~\" * (SEQ_LENGTH - len(word))).lower()\n",
    "\n",
    "    sequence = array([mapping[char] for char in line])\n",
    "    sequence = array([to_categorical(x, num_classes=vocab_size) for x in array(sequence)])\n",
    "    s = np.empty([1, sequence.shape[0], sequence.shape[1]])\n",
    "    s[0] = sequence\n",
    "\n",
    "\n",
    "    return model.predict(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00487126 0.99512875]]\n",
      "[[0.6701656  0.32983443]]\n",
      "[[0.976143   0.02385698]]\n",
      "[[0.64114374 0.35885626]]\n"
     ]
    }
   ],
   "source": [
    "print(testWord(\"TAAR1\", model, 1))\n",
    "print(testWord(\"once turtle was\", model, 1))\n",
    "print(testWord(\"Tyrosine\", model, 1))\n",
    "print(testWord(\"alanine glycin\", model, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
